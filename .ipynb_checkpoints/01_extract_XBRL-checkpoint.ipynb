{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extracting Data from Companies House Electronic Records\n",
    "\n",
    "Companies house receives 75% of its records in XBRL or iXBRL format, a glorified tagged xml document that should allow for easy automated extraction of statistics.\n",
    "\n",
    "The software in this repo was developed after reading of this (American) example:\n",
    "https://www.codeproject.com/Articles/1227765/Parsing-XBRL-with-Python\n",
    "\n",
    "The functions for doing so are hosted in the module xbrl_parser.py\n",
    "\n",
    "Both xbrl_parser.py and this script have a number of python package dependencies so expect to have to install some things.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Returned dict schema for html/xml sourced data\n",
    "\n",
    "A practical note:  Apart from explicitly elevated metadata, all extracted values are stored in a list of \"elements\" within the returned dict.  Each element is itself a dict, containing the name and value of the discovered data along with fields unit and date for metadata."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup (import modules, set up a helper function for getting filepaths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xbrl_parser as xp\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import importlib\n",
    "\n",
    "def get_filepaths(directory):\n",
    "\n",
    "    \"\"\" Helper function - \n",
    "    Get all of the filenames in a directory that\n",
    "    end in htm* or xml.\n",
    "    Under the assumption that all files within\n",
    "    the folder are financial records. \"\"\"\n",
    "\n",
    "    files = [directory + \"/\" + filename\n",
    "                for filename in os.listdir(directory)\n",
    "                    if ((\"htm\" in filename.lower()) or (\"xml\" in filename.lower())) ]\n",
    "    return(files)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extracting data from documents\n",
    "\n",
    "We'll import the module, and process some files\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['./example_data_XBRL_iXBRL/Prod224_0042_00958610_20160930.xml',\n",
       " './example_data_XBRL_iXBRL/Prod223_2125_09749826_20170831.html',\n",
       " './example_data_XBRL_iXBRL/Prod223_2125_09170142_20170831.html',\n",
       " './example_data_XBRL_iXBRL/Prod224_0042_03237381_20160831.xml',\n",
       " './example_data_XBRL_iXBRL/Prod223_2125_09900460_20161231.html',\n",
       " './example_data_XBRL_iXBRL/Prod223_2125_09652609_20180331.html',\n",
       " './example_data_XBRL_iXBRL/Prod223_2125_09722743_20170831.html']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get all the filenames from the example folder\n",
    "files = get_filepaths(\"./example_data_XBRL_iXBRL\")\n",
    "\n",
    "# There's 379 examples currently\n",
    "files[0:7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reload the xbrl_parser module (don't need this normally, it's just useful for me\n",
    "# for iterative testing of changes)\n",
    "importlib.reload(xp)\n",
    "\n",
    "# try getting the first file (an XML, or XBRL, file)\n",
    "doc = xp.process_account(files[0])\n",
    "\n",
    "# display for fun\n",
    "doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# try getting the second file (an HTML, or iXBRL, file)\n",
    "doc2 = xp.process_account(files[1])\n",
    "\n",
    "# display for fun\n",
    "doc2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Retrieve elements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop through the document, retrieving any element with a matching name\n",
    "for element in doc['elements']:\n",
    "    if element['name'] == 'netassetsliabilitiesincludingpensionassetliability':\n",
    "        print(element)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the all the data to long-thin table format for use with SQL\n",
    "# Note, tables from docs should be appendable to one another to create\n",
    "# tables of all data\n",
    "xp.flatten_data(doc).head(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Finally, build a table of all variables from all example (digital) documents\n",
    "# This can take a while\n",
    "\n",
    "# Empty table awaiting results\n",
    "results = pd.DataFrame()\n",
    "\n",
    "# For every file\n",
    "for file in files:\n",
    "    \n",
    "    # Read the file\n",
    "    doc = xp.process_account(file)\n",
    "    \n",
    "    # tabulate the results\n",
    "    doc_df = xp.flatten_data(doc)\n",
    "    \n",
    "    # append to table\n",
    "    results = results.append(doc_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results.head(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's ~380 files extracted to obtain ~ 22,000 variables - on average 60 variables per record.  As you've just seen though, extraction can take a while!  Searching through the documents using BeautifulSoup can take a long time, especially where chasing element links to get information on units.  Hopefully this is the sort of thing that can be optimised in future, or it'll be rendered irrelevant by Moore's Law."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results.to_csv(\"example_extracted_XBRL_data.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get summary variables\n",
    "\n",
    "These I've implemented to work off the MongoDB/Dict representation of the data that the scraping code returns.  It's assumed that if you wish to work with the \"flattened\" SQL-compatible data instead you can develop your own queries :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = 3\n",
    "doc = xp.process_account(files[index])\n",
    "\n",
    "# This tries to add up every variable it can find in a list of variable names\n",
    "test = xp.summarise_by_sum(doc, [\"fixedassets\",\n",
    "                                 \"currentassets\",\n",
    "                                 \"intangibleassets\",\n",
    "                                 \"tangiblefixedassets\",\n",
    "                                 \"intangiblefixedassets\",\n",
    "                                 \"investmentsfixedassets\",\n",
    "                                 \"cashbankinhand\",\n",
    "                                 \"cashbankonhand\",\n",
    "                                 \"cashbank\",\n",
    "                                 \"cashonhand\",\n",
    "                                 \"cashinhand\",\n",
    "                                 \"calledupsharecapitalnotpaidnotexpressedascurrentasset\",\n",
    "                                 \"otherdebtors\"])\n",
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This returns the first variable it finds in a prioritised list\n",
    "# Here I've gone looking for net assets/liabilities\n",
    "test = xp.summarise_by_priority(doc, [\"netassetsliabilitiesincludingpensionasset\",\n",
    "                                      \"netassetsliabilityexcludingpensionasset\",\n",
    "                                      \"netassetsliabilities\",\n",
    "                                      \"totalassetslesscurrentliabilities\",\n",
    "                                      \"netcurrentassetsliabilities\"])\n",
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here I've applied it to shareholder funds/equity\n",
    "test = xp.summarise_by_priority(doc, [\"shareholderfunds\",\n",
    "                                      \"equity\",\n",
    "                                      \"capitalandreserves\"])\n",
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This one just tries to return all named variables\n",
    "test = xp.summarise_set(doc, [\"creditors\",\n",
    "                              \"debtors\",\n",
    "                              'accountstypefullorabbreviated',\n",
    "                              'descriptionprincipalactivities',\n",
    "                              'accountingstandardsapplied',\n",
    "                              'entitytradingstatus'])\n",
    "test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finding consolidated status\n",
    "\n",
    "It turns out that \"consolidated\" will be tricky - variables exist for it in xbrl/ixbrl docs but they're missleading.  A lot of companies have such a variable and then set the value to \"False\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "allvars = pd.read_csv(\"all_variables_sources.csv\")\n",
    "\n",
    "allvars.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "consolidation_vars = allvars[allvars['Element Name'].str.contains(\"consoli\")]\n",
    "consolidation_vars.sort_values(\"Element Count\", ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "consolidation_vars = allvars[allvars['Element Name'].str.contains(\"parent\")]\n",
    "consolidation_vars.sort_values(\"Element Count\", ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "consolidation_vars = allvars[allvars['Element Name'].str.contains(\"accou\")]\n",
    "consolidation_vars.sort_values(\"Element Count\", ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "consolidation_vars = allvars[allvars['Element Name'].str.contains(\"activit\")]\n",
    "consolidation_vars.sort_values(\"Element Count\", ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# manual filter to relevant vars from remaining\n",
    "for each in consolidation_vars['Element Name']:\n",
    "    print(each)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Built a truth table; for each variable what value should it have to mark a company as consolidated?\n",
    "consolidation_var_table = {\n",
    "    \"includedinconsolidationsubsidiary\":True,\n",
    "    \"investmententityrequiredtoapplyexceptionfromconsolidationtruefalse\":True,\n",
    "    \"subsidiaryunconsolidatedtruefalse\":False,\n",
    "    \"descriptionreasonwhyentityhasnotpreparedconsolidatedfinancialstatements\":\"exist\",\n",
    "    \"consolidationpolicy\":\"exist\",\n",
    "    \"nameparententity\":\"exist\",\n",
    "    \"amountsowedtoparentundertakingwithinoneyear\":\"exist\",\n",
    "    \"scopeaccounts\":\"exist\"\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pymongo\n",
    "\n",
    "cl = pymongo.MongoClient()\n",
    "db = cl['CH_records']\n",
    "col = db['digital_record_scrapes']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/martin/.local/lib/python3.6/site-packages/ipykernel_launcher.py:1: DeprecationWarning: count is deprecated. Use Collection.count_documents instead.\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "3015767"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "col.find().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = col.find().limit(300000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = pd.DataFrame()\n",
    "\n",
    "for doc in docs:\n",
    "    \n",
    "    for each in doc['elements']:\n",
    "    \n",
    "        if each['name'] in consolidation_var_table.keys():\n",
    "        \n",
    "            res = {\"record\":doc['doc_companieshouseregisterednumber'],\n",
    "                   \"variable\":each['name'],\n",
    "                   \"value\":each['value'],\n",
    "                   \"truthval\":consolidation_var_table[each['name']]}\n",
    "        \n",
    "            results = results.append(res, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results['variable'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results[['variable', 'value', 'record']].groupby(['variable', 'value']).agg('count').to_csv(\"example_consolidation_variables.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results[['variable', 'value', 'record']].groupby(['variable', 'value']).agg('count')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extracting principal activities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000\n",
      "created_data\n",
      "20000\n",
      "created_data\n",
      "30000\n",
      "created_data\n",
      "40000\n",
      "created_data\n"
     ]
    }
   ],
   "source": [
    "docs = col.find(no_cursor_timeout=True)\n",
    "\n",
    "descriptions = pd.DataFrame()\n",
    "\n",
    "counter = 0\n",
    "created = 0\n",
    "\n",
    "for doc in docs:\n",
    "    \n",
    "    try:\n",
    "        for each in doc['elements']:\n",
    "    \n",
    "            if each['name'] == \"descriptionprincipalactivities\":\n",
    "        \n",
    "                res = {\"companieshouseregisterednumber\":doc['doc_companieshouseregisterednumber'],\n",
    "                       \"balancesheetdate\":doc['doc_balancesheetdate'],\n",
    "                       \"descriptionprincipalactivities\":each['value']}\n",
    "        \n",
    "                descriptions = descriptions.append(res, ignore_index=True)\n",
    "                \n",
    "                counter = counter + 1\n",
    "                \n",
    "                if counter % 10000 == 0:\n",
    "                    print(counter)\n",
    "                    \n",
    "                    if created == 1:\n",
    "                        with open(\"companieshousedescriptions.csv\", \"a\") as f:\n",
    "                            descriptions.to_csv(f, header=False)\n",
    "                            print(\"appended data\")\n",
    "                            \n",
    "                    created = 1\n",
    "                    \n",
    "                    else:\n",
    "                        with open(\"companieshousedescriptions.csv\", \"w\") as f:\n",
    "                            descriptions.to_csv(f)\n",
    "                            print(\"created_data\")\n",
    "                    \n",
    "                    descriptions = pd.DataFrame()\n",
    "                    \n",
    "                break\n",
    "    \n",
    "        \n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "descriptions.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "descriptions.to_csv(\"company_descriptions.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(descriptions['descriptionprincipalactivities'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
